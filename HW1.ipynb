{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.sparse import dok_matrix, coo_matrix, csr_matrix, csc_matrix\n",
    "from scipy.sparse.linalg import norm\n",
    "\n",
    "import time\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/vocab-15kws.txt') as f:\n",
    "    V = [l.strip() for l in f.readlines()]\n",
    "    \n",
    "with open('./data/vocab-5k.txt') as f:\n",
    "    V_c = [l.strip() for l in f.readlines()]\n",
    "    \n",
    "word_to_ind = dict(zip(V, range(len(V))))\n",
    "contextword_to_ind = dict(zip(V_c, range(len(V_c))))\n",
    "\n",
    "#with open('./hw1-data-31190-fall-2020/wiki-1percent.txt') as f:\n",
    "#    data = [l.strip() for l in f.readlines()]\n",
    "    \n",
    "with open('./data/men.txt') as f:\n",
    "    men = f.readlines()[1:]\n",
    "    \n",
    "with open('./data/simlex-999.txt') as f:\n",
    "    simlex = f.readlines()[1:]\n",
    "    \n",
    "def parse_scores(lines):\n",
    "    counts = {}\n",
    "    for line in lines:\n",
    "        w1, w2, n = line.split()\n",
    "        n = float(n)\n",
    "        counts[w1, w2] = n\n",
    "    return counts\n",
    "\n",
    "men, simlex = parse_scores(men), parse_scores(simlex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', '.', ',', 'of', 'and'] \n",
      " ['the', '.', ',', 'of', 'and']\n"
     ]
    }
   ],
   "source": [
    "print(V[:5], '\\n', V_c[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 (12 points) Implement distributional counting as described above for a provided w, V, and VC. Submit your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 997898 # len(data)\n",
    "\n",
    "def get_vectors_and_idf(V, V_c, windows=[1, 3, 6]):\n",
    "    \"\"\"\n",
    "        txt: a list of sentence strings\n",
    "        V: a list of vocabulary words\n",
    "        V_c: a list of context words\n",
    "        window: an integer, window length\n",
    "    \"\"\"\n",
    "    idf = {}\n",
    "    word_to_ind = dict(zip(V, range(len(V))))\n",
    "    contextword_to_ind = dict(zip(V_c, range(len(V_c))))\n",
    "    df_v, df_v_c = dok_matrix((len(V), 1), dtype=np.float32), dok_matrix((len(V_c), 1), dtype=np.float32)\n",
    "    t1 = time.time()\n",
    "    vectors = {k: dok_matrix((len(V), len(V)), dtype=np.float32) for k in [('V', 1), ('V', 3), ('V', 6)]}\n",
    "    for k in [('V_c', 1), ('V_c', 3), ('V_c', 6)]:\n",
    "        vectors[k] = dok_matrix((len(V), len(V_c)), dtype=np.float32)\n",
    "    with open('./data/wiki-1percent.txt') as f:\n",
    "        for linenum, line in tqdm(enumerate(f)):\n",
    "            if (linenum+1)%50000==0 or linenum==0:\n",
    "                t2 = time.time()\n",
    "                print(f\"({linenum+1}/{l}) : {t2-t1} s\")\n",
    "                #with open(f'/Users/aabir/Documents/uchicago/nlp/hw1/vectors2.pkl', 'wb') as f:\n",
    "                #    pickle.dump(vectors, f)\n",
    "                #with open(f'/Users/aabir/Documents/uchicago/nlp/hw1/idf2.pkl', 'wb') as f:\n",
    "                #    pickle.dump(idf, f)\n",
    "            line = line.split()\n",
    "            for word_ind, word in enumerate(line):\n",
    "                if word not in V:\n",
    "                    continue\n",
    "                df_v[word_to_ind[word]] = df_v[word_to_ind[word]] + 1\n",
    "                if word in V_c:\n",
    "                    df_v_c[contextword_to_ind[word]] = df_v_c[contextword_to_ind[word]]+1\n",
    "                continue\n",
    "                for w in windows:\n",
    "                    for cword_ind in range(word_ind-w, word_ind+w):\n",
    "                        if cword_ind != word_ind and cword_ind <= len(line)-1 and cword_ind >= 0:\n",
    "                            contextword = line[cword_ind]\n",
    "                            if contextword in V_c:\n",
    "                                i, j = word_to_ind[word], contextword_to_ind[contextword]\n",
    "                                vectors['V_c', w][i, j] = vectors['V_c', w][i, j] + 1\n",
    "                            if contextword in V:\n",
    "                                i, j = word_to_ind[word], word_to_ind[contextword]\n",
    "                                vectors['V', w][i, j] = vectors['V', w][i, j] + 1\n",
    "    idf['V'] = df_v\n",
    "    idf['V_c'] = df_v_c\n",
    "    #df_v, df_v_c = df_v.toarray().flatten(), df_v_c.toarray().flatten()\n",
    "    #idf['V'] = np.array([l/x if x!=0 else 0. for x in df_v])\n",
    "    #idf['V_c'] = np.array([l/x if x!=0 else 0. for x in df_v_c])\n",
    "    return vectors, idf\n",
    "\n",
    "def get_idf(V, V_c, startind=0, lastind=np.inf):\n",
    "    \"\"\"\n",
    "        txt: a list of sentence strings\n",
    "        V: a list of vocabulary words\n",
    "        V_c: a list of context words\n",
    "        window: an integer, window length\n",
    "    \"\"\"\n",
    "    idf = {}\n",
    "    word_to_ind = dict(zip(V, range(len(V))))\n",
    "    contextword_to_ind = dict(zip(V_c, range(len(V_c))))\n",
    "    V, V_c = set(V), set(V_c)\n",
    "    df_v, df_v_c = dok_matrix((len(V), 1), dtype=np.float32), dok_matrix((len(V_c), 1), dtype=np.float32)\n",
    "    t1 = time.time()\n",
    "    with open('./hw1-data-31190-fall-2020/wiki-1percent.txt') as f:\n",
    "        for linenum, line in tqdm(enumerate(f)):\n",
    "            if linenum<startind:\n",
    "                continue\n",
    "            if (linenum+1)%50000==0 or linenum==0:\n",
    "                t2 = time.time()\n",
    "                print(f\"({linenum+1}/{l}) : {t2-t1} s\")\n",
    "                #with open(f'/Users/aabir/Documents/uchicago/nlp/hw1/vectors2.pkl', 'wb') as f:\n",
    "                #    pickle.dump(vectors, f)\n",
    "                #with open(f'/Users/aabir/Documents/uchicago/nlp/hw1/idf2.pkl', 'wb') as f:\n",
    "                #    pickle.dump(idf, f)\n",
    "            line = line.split()\n",
    "            for word_ind, word in enumerate(line):\n",
    "                if word not in V:\n",
    "                    continue\n",
    "                df_v[word_to_ind[word]] = df_v[word_to_ind[word]] + 1\n",
    "                if word in V_c:\n",
    "                    df_v_c[contextword_to_ind[word]] = df_v_c[contextword_to_ind[word]]+1\n",
    "                continue\n",
    "            if lastind==linenum:\n",
    "                    return df_v, df_v_c\n",
    "    #idf['V'] = np.array([l/x if x!=0 else 0. for x in df_v])\n",
    "    #idf['V_c'] = np.array([l/x if x!=0 else 0. for x in df_v_c])\n",
    "    return df_v, df_v_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: my computer was struggling to run this so I ran it on RCC and downloaded the Pickled results for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loadpickle = True\n",
    "\n",
    "counts, idfs = {}, {}\n",
    "\n",
    "if loadpickle:\n",
    "    vectors, dfs = [pickle.load(open(f'/Users/aabir/Documents/uchicago/nlp/hw1/{f}.pkl', 'rb')) \n",
    "                    for f in ['vectors', 'dfs']]\n",
    "    dfs = {'V':dfs[0], 'V_c':dfs[1]}\n",
    "else:\n",
    "    vectors, dfs = get_vectors_and_idf(V, V_c, [1, 3, 6])\n",
    "    if input('Overwrite pickles? y/n\\t').lower()=='y':\n",
    "        with open(f'/Users/aabir/Documents/uchicago/nlp/hw1/vectors.pkl', 'wb') as f:\n",
    "            pickle.dump(vectors, f)\n",
    "        with open(f'/Users/aabir/Documents/uchicago/nlp/hw1/dfs.pkl', 'wb') as f:\n",
    "            pickle.dump(dfs, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs = {}\n",
    "for k, v in dfs.items():\n",
    "    idfs[k] = np.array([l/val if val!=0 else 0 for val in v.toarray()]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141228700.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors['V', 6].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 (6 points) Using vocab-15kws.txt to populate V and vocab-5k.txt to populate VC, use your code to report #(x, y) for the pairs in the following table for both w = 3 and w = 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second time I ran my code, these values did not match up due to some changes (I modified it to run the whole ensemble with different context vocabulary and window sizes). However, I  ran it just for V_c, w=3 once previously and found counts that matched the expected value. I'm printing them here:\n",
    "\n",
    "```\n",
    "print(\"for w=3:\\n\")\n",
    "print(counts[('chicken', 'the')])\n",
    "print(counts[('chicken', 'wings')])\n",
    "print(counts[('chicago', 'chicago')])\n",
    "print(counts[('coffee', 'the')])\n",
    "print(counts[('coffee', 'cup')])\n",
    "print(counts[('coffee', 'coffee')])\n",
    ">>> 52\n",
    ">>> 6\n",
    ">>> 38\n",
    ">>> 95\n",
    ">>> 10\n",
    ">>> 4\n",
    "```\n",
    "\n",
    "I'm printing the results that don't match up (from the latest run) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for w=3:\n",
      "\n",
      "('chicken', 'the') : 40.0\n",
      "('chicken', 'wings') : 4.0\n",
      "('chicago', 'chicago') : 22.0\n",
      "('coffee', 'the') : 70.0\n",
      "('coffee', 'cup') : 8.0\n",
      "('coffee', 'coffee') : 3.0\n"
     ]
    }
   ],
   "source": [
    "print(\"for w=3:\\n\")\n",
    "for (w1, w2) in [('chicken', 'the'), ('chicken', 'wings'), ('chicago', 'chicago'),\n",
    "             ('coffee', 'the'), ('coffee', 'cup'), ('coffee', 'coffee')]:\n",
    "    i, j = word_to_ind[w1], contextword_to_ind[w2]\n",
    "    print((w1, w2), ':', vectors['V_c', 3][i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for w=6:\n",
      "\n",
      "('chicken', 'the') : 83.0\n",
      "('chicken', 'wings') : 5.0\n",
      "('chicago', 'chicago') : 101.0\n",
      "('coffee', 'the') : 175.0\n",
      "('coffee', 'cup') : 11.0\n",
      "('coffee', 'coffee') : 29.0\n"
     ]
    }
   ],
   "source": [
    "print(\"for w=6:\\n\")\n",
    "for (w1, w2) in [('chicken', 'the'), ('chicken', 'wings'), ('chicago', 'chicago'),\n",
    "             ('coffee', 'the'), ('coffee', 'cup'), ('coffee', 'coffee')]:\n",
    "    i, j = word_to_ind[w1], contextword_to_ind[w2]\n",
    "    print((w1, w2), ':', vectors['V_c', 6][i, j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 (6 points) Using w = 3 (and again using vocab-15kws.txt for V and vocab-5k.txt for VC ), eval- uate your count-based word vectors using EVALWS and report your results on MEN and SimLex-999. As a sanity check, your Spearman correlation for MEN should be close to 0.22."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    n1, n2 = norm(vec1), norm(vec2)\n",
    "    if n1*n2 != 0:\n",
    "        return vec1.dot(vec2.T).toarray()[0, 0]/n1/n2\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def evalws(vectors, benchmark, name):\n",
    "    cosinevals_dict = {}\n",
    "    vectors = vectors.tocsr()\n",
    "    vec_cos, benchmark_cos = [], []\n",
    "    for (w1, w2), sim in tqdm(benchmark.items()):\n",
    "        if w1 not in V or w2 not in V:\n",
    "            vec_cos.append(0.)\n",
    "        else:\n",
    "            w1i, w2i = word_to_ind[w1], word_to_ind[w2]\n",
    "            cos = cosine_similarity(vectors[w1i], vectors[w2i])\n",
    "            vec_cos.append(cos)\n",
    "            #print(cos, sim)\n",
    "        benchmark_cos.append(sim)\n",
    "    #print(vec_cos)\n",
    "    #print(benchmark_cos)\n",
    "    rho, pval = spearmanr(vec_cos, benchmark_cos)\n",
    "    print(f\"{name} spearmanr: rho = {rho}, pval = {pval}\")\n",
    "    return rho, pval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EvalWS for w=3, V as vocab, V_c as context vocabulary with **raw counts** as vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEN spearmanr: rho = 0.2203416545296961, pval = 2.6173677002997682e-34\n",
      "SIMLEX spearmanr: rho = 0.05480605099111539, pval = 0.08338305067574583\n"
     ]
    }
   ],
   "source": [
    "_ = evalws(vectors['V_c', 3], men, 'MEN')\n",
    "_ = evalws(vectors['V_c', 3], simlex, 'SIMLEX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sun, sunlight: 0.8683664202690125\n",
      "automobile, car: 0.8751132488250732\n",
      "festival, whiskers: 0.5750707387924194\n",
      "bakery, zebra: 0.8923996090888977\n"
     ]
    }
   ],
   "source": [
    "def check_cosine_similarity(w1, w2, vecs):\n",
    "    w1i, w2i = word_to_ind[w1], word_to_ind[w2]\n",
    "    cos = cosine_similarity(vecs[w1i], vecs[w2i])\n",
    "    print(f\"{w1}, {w2}: {cos}\")\n",
    "    \n",
    "check_cosine_similarity('sun', 'sunlight', vectors['V_c', 3])\n",
    "check_cosine_similarity('automobile', 'car', vectors['V_c', 3])\n",
    "check_cosine_similarity('festival', 'whiskers', vectors['V_c', 3])\n",
    "check_cosine_similarity('bakery', 'zebra', vectors['V_c', 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 (10 points) Extend your implementation to be able to compute IDF-based word vectors using Eq. 1. Us- ing w = 3, vocab-15kws.txt to populate V , and vocab-5k.txt to populate VC , evaluate (EVALWS) your IDF-based word vectors and report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idf_vecs(vecs, idf, method='csc'):\n",
    "    print('computing TF-IDF vectors')\n",
    "    if method=='csr':\n",
    "        return vecs.tocsr().multiply(idf)\n",
    "    if method=='csc':\n",
    "        return vecs.tocsc().multiply(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing TF-IDF vectors\n",
      "7.508224010467529\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "idf_vecs = get_idf_vecs(vectors['V_c', 3], idfs['V_c'], 'csc')\n",
    "print(time.time()-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15228, 5000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEN spearmanr: rho = 0.44886867503643035, pval = 1.0831563340443806e-148\n",
      "SIMLEX spearmanr: rho = 0.1855666928945375, pval = 3.436639200620402e-09\n"
     ]
    }
   ],
   "source": [
    "_ = evalws(dok_matrix(idf_vecs), men, \"MEN\")\n",
    "_ = evalws(dok_matrix(idf_vecs), simlex, \"SIMLEX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 (8 points) Implement the capability of computing PMIs. Use your code to calculate PMIs for w = 3 when using vocab-15kws.txt to populate V and vocab-5k.txt to populate VC . Note that since we are using different vocabularies for center words and context words, pmi(a, b) will not necessarily equal pmi(b, a) (though they will be similar). (If there is a word in V that has no counts, the numerator and denominator for all of its PMI values will be zero, so you can just define all such PMIs to be zero.) For center word x = “coffee”, print the 10 context words with the largest PMIs and the 10 context words with the smallest PMIs. Print both the words and the PMI values. (Note: using my implementation, the highest-PMI context word was “tea” with PMI 8.166, and the lowest-PMI context word was “he” with PMI -2.26034.)\n",
    "\n",
    "\n",
    "### 3.2 (6 points) Now, define word vectors using PMI. That is, the word vector for a word x ∈ V has an entry for each word y ∈ VC with value given by pmi(x, y). As above, use w = 3, vocab-15kws.txt to populate V , and vocab-5k.txt to populate VC . Evaluate (EVALWS) your PMI-based word vectors and report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_v_c3 = vectors['V_c', 3].sum()\n",
    "N_v3 = vectors['V', 3].sum()\n",
    "def get_pmi_for_words(w1, w2, vecs, N, contextvocab='V_c'):\n",
    "    i = word_to_ind[w1]\n",
    "    j = contextword_to_ind[contextword] if contextvocab=='V_c' else word_to_ind[w2]\n",
    "    vecs = vecs.tolil()\n",
    "    if vecs[i,j]!=0:\n",
    "        return np.log2(vecs[i,j])+np.log2(N)-np.log2(vecs[i,:].sum()*vecs[:,j].sum())\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_pmis = []\n",
    "for contextword in V_c:\n",
    "    coffee_pmis.append(get_pmi_for_words('coffee', contextword, vectors['V_c', 3], N_v_c3))\n",
    "    \n",
    "args = np.argsort(coffee_pmis)\n",
    "minwords, maxwords = [V_c[i] for i in args[:10]], [V_c[i] for i in args[-10:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'coffee' min PMI words:\n",
      "\tthis: -2.6687450408935547\n",
      "\the: -1.987762451171875\n",
      "\tbe: -1.7789745330810547\n",
      "\thad: -1.6826629638671875\n",
      "\tnot: -1.5677719116210938\n",
      "\tits: -1.5174312591552734\n",
      "\tafter: -1.3077754974365234\n",
      "\tother: -1.2900466918945312\n",
      "\tall: -1.284261703491211\n",
      "\t;: -1.2233772277832031\n",
      "\n",
      "'coffee' max PMI words:\n",
      "\ttea: 8.36812973022461\n",
      "\tshop: 7.819431304931641\n",
      "\tdrinking: 7.684116363525391\n",
      "\tshops: 7.584140777587891\n",
      "\tcosta: 7.232067108154297\n",
      "\tcoffee: 6.40800666809082\n",
      "\tseattle: 6.275318145751953\n",
      "\thouses: 6.181125640869141\n",
      "\tlloyd: 6.114559173583984\n"
     ]
    }
   ],
   "source": [
    "print(f\"'coffee' min PMI words:\")\n",
    "for i in args[:10]:\n",
    "    print(f\"\\t{V_c[i]}: {coffee_pmis[i]}\")\n",
    "\n",
    "print(f\"\\n'coffee' max PMI words:\")\n",
    "for i in args[-1:-10:-1]:\n",
    "    print(f\"\\t{V_c[i]}: {coffee_pmis[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pmi_vectors(vecs, N=None):\n",
    "    print('computing PMI vectors')\n",
    "    pmi_vecs = dok_matrix(vecs.shape, dtype=np.float32)\n",
    "    if N is None:\n",
    "        N = vecs.sum()\n",
    "    vecs = vecs.tolil()\n",
    "    rowsums, colsums = vecs.sum(axis=1).flatten().T, vecs.sum(axis=0).flatten().T\n",
    "    vecs = vecs.tocoo()\n",
    "    for i, j, v in tqdm(zip(vecs.row, vecs.col, vecs.data)):\n",
    "        pmi_vecs[i, j] = np.log2(v*N/(rowsums[i]*colsums[j]))\n",
    "    return pmi_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#13:13 pm Oct 18\n",
    "pmi_vecs = get_pmi_vectors(vectors['V_c', 3], N_v_c3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EvalWS for w=3, V as vocab, V_c as context vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEN spearmanr: rho = 0.46153842598967837, pval = 3.549880350180334e-158\n",
      "SIMLEX spearmanr: rho = 0.19900072623832812, pval = 2.2170010905021224e-10\n"
     ]
    }
   ],
   "source": [
    "_ = evalws(pmi_vecs, men, 'MEN')\n",
    "_ = evalws(pmi_vecs, simlex, 'SIMLEX')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 (8 points) Evaluate the word vectors (EVALWS) corresponding to the three ways of comput- ing vectors (counts, IDF, and PMI), three values of w (1, 3, and 6), and two context vocabularies (vocab-15kws.txt and vocab-5k.txt). For all cases, use vocab-15kws.txt for V . Report the results (there should be 36 correlations in all) and describe your findings. What happens as window size changes for different methods of creating word vectors? What happens when context vocabulary changes? Why do you think you observe the trends you see? Do you see the same trends for MEN and SimLex or do they differ?\n",
    "\n",
    "\n",
    "### 4.2 (4 points) You should observe systematic trends in terms of correlation as window size changes which should differ for MEN and SimLex-999. Look at some of the manually-annotated similarities in the MEN and SimLex-999 datasets and describe why you think the two datasets show the trends they do. Are these two datasets encoding the same type of similarity? How does the notion of similarity differ between them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "V_c, MEN, 1, raw\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743a29d736144cf8892072be76ac676b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MEN spearmanr: rho = 0.1453646412214026, pval = 1.2324872951126856e-15\n",
      "\n",
      "V_c, SIMLEX, 1, raw\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a534446bc94682a6161a0e50ee210f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=999.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SIMLEX spearmanr: rho = 0.05130971677729792, pval = 0.10506450447826148\n",
      "computing TF-IDF vectors\n",
      "\n",
      "V_c, MEN, 1, tfidf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc92f148b9a4c88a92cd279caf3aecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MEN spearmanr: rho = 0.29421919712354866, pval = 5.612330898012024e-61\n",
      "\n",
      "V_c, SIMLEX, 1, tfidf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc7b6310d6804799b04030eb0352666d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=999.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SIMLEX spearmanr: rho = 0.17777717010466934, pval = 1.5396835417652116e-08\n",
      "computing PMI vectors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b638d8c710d34f58a565a5250b8d6c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "V_c, MEN, 1, pmi\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "243ce92d52aa46299c5de1af4189e6d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MEN spearmanr: rho = 0.3622824597101258, pval = 1.0183168811957188e-93\n",
      "\n",
      "V_c, SIMLEX, 1, pmi\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e15cc38b7c8f415d91c5106276f68de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=999.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SIMLEX spearmanr: rho = 0.20832696064990097, pval = 2.942379302716302e-11\n",
      "\n",
      "V_c, MEN, 6, raw\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cffddd05d7a4fee9b26843a4ff5322e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MEN spearmanr: rho = 0.2405388357214604, pval = 9.544733802667223e-41\n",
      "\n",
      "V_c, SIMLEX, 6, raw\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c85864235c34ce08217cf3dba8fdb49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=999.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SIMLEX spearmanr: rho = 0.04655659821922139, pval = 0.14143513010068376\n",
      "computing TF-IDF vectors\n",
      "\n",
      "V_c, MEN, 6, tfidf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3198bf7be4ec4f619d151e610adbc01d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MEN spearmanr: rho = 0.5196876901148884, pval = 3.170819001168843e-207\n",
      "\n",
      "V_c, SIMLEX, 6, tfidf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb0e6dfb2e944b4a5892ea6018b8f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=999.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SIMLEX spearmanr: rho = 0.11878414885025199, pval = 0.0001678633374868749\n",
      "computing PMI vectors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9261eb9287184c4499b2455d3cd31359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "V_c, MEN, 6, pmi\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0111cc2976fa46a383f574eeb52684a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MEN spearmanr: rho = 0.4723553399745761, pval = 1.3478358479482169e-166\n",
      "\n",
      "V_c, SIMLEX, 6, pmi\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a32aa22d04684072a0b972c8086db310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=999.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SIMLEX spearmanr: rho = 0.154171537123257, pval = 9.776058133618332e-07\n",
      "\n",
      "V, MEN, 1, raw\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "759194f946594e95b77889278cf6a1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MEN spearmanr: rho = 0.14000684517560627, pval = 1.3263418238349479e-14\n",
      "\n",
      "V, SIMLEX, 1, raw\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e34e333463743a9b0b75b24b05058b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=999.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SIMLEX spearmanr: rho = 0.05250729745927539, pval = 0.09718379879864335\n",
      "\n",
      "V, MEN, 1, tfidf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2acaaac8f44d4e3aa0aff7f8013ab82e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MEN spearmanr: rho = 0.14000684517560627, pval = 1.3263418238349479e-14\n",
      "\n",
      "V, SIMLEX, 1, tfidf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "995954558a1743898b8624dd9ec21f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=999.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SIMLEX spearmanr: rho = 0.05250729745927539, pval = 0.09718379879864335\n",
      "computing PMI vectors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c5233c5b764339aaca6d7eabc82807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "V, MEN, 1, pmi\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b8ab1b0afc4e67b8748c81b32047f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MEN spearmanr: rho = 0.38785047256811156, pval = 2.7867884669220316e-108\n",
      "\n",
      "V, SIMLEX, 1, pmi\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1219ce6e354243648119856913c2f967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=999.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SIMLEX spearmanr: rho = 0.23303023453015143, pval = 8.712874018535476e-14\n",
      "\n",
      "V, MEN, 3, raw\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdaadbd0c3ed4271a4ef336475b21128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MEN spearmanr: rho = 0.21551090456727312, pval = 7.352005167777831e-33\n",
      "\n",
      "V, SIMLEX, 3, raw\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8f0e8e0fce4af282e1571c82b63427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=999.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SIMLEX spearmanr: rho = 0.05361097703127073, pval = 0.09034575849779028\n",
      "\n",
      "V, MEN, 3, tfidf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e990be8e2a4ee5821dc947748d1a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MEN spearmanr: rho = 0.21551090456727312, pval = 7.352005167777831e-33\n",
      "\n",
      "V, SIMLEX, 3, tfidf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd85ba72e1b448e993559ad8b1a685ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=999.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SIMLEX spearmanr: rho = 0.05361097703127073, pval = 0.09034575849779028\n",
      "computing PMI vectors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "985d51fbc51a4630b74921f33a668b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "V, MEN, 3, pmi\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bafdf4b6aed742238ccd78b94d4a7d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MEN spearmanr: rho = 0.5224845465220976, pval = 7.833857588927124e-210\n",
      "\n",
      "V, SIMLEX, 3, pmi\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ffa2dac84ca429d91be74229c14b1b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=999.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SIMLEX spearmanr: rho = 0.22330635121502992, pval = 9.364300306845717e-13\n",
      "\n",
      "V, MEN, 6, raw\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742ce1f61f0f48db972336590c362a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MEN spearmanr: rho = 0.23572152690209489, pval = 3.731207056287394e-39\n",
      "\n",
      "V, SIMLEX, 6, raw\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09dec5726af5430593add9df35b2f686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=999.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SIMLEX spearmanr: rho = 0.04227952379792869, pval = 0.18179411276008164\n",
      "\n",
      "V, MEN, 6, tfidf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da910b5ddd7459190bd799ad2433a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MEN spearmanr: rho = 0.23572152690209489, pval = 3.731207056287394e-39\n",
      "\n",
      "V, SIMLEX, 6, tfidf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdba2bf84eab4b67921b76f836034462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=999.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SIMLEX spearmanr: rho = 0.04227952379792869, pval = 0.18179411276008164\n",
      "computing PMI vectors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3344aca95ea14e3ca5581f82f9b46d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "V, MEN, 6, pmi\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abdc327849b04596a2feda125700eebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MEN spearmanr: rho = 0.5259332859986898, pval = 4.418948548753325e-213\n",
      "\n",
      "V, SIMLEX, 6, pmi\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5891b26c5c4d4475928ec34e2bb50bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=999.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SIMLEX spearmanr: rho = 0.16213999988025068, pval = 2.570823255684684e-07\n"
     ]
    }
   ],
   "source": [
    "columns = ['rho', 'pval', 'context_vocab', 'windowsize', 'eval_data', 'vector_type']\n",
    "\n",
    "scores_df = pd.DataFrame({}, columns=columns)\n",
    "\n",
    "for vocab in ['V_c', 'V']:\n",
    "    for window in [1, 3, 6]:\n",
    "        if (vocab, window) == ('V_c', 3):\n",
    "            continue\n",
    "        N = vectors[vocab, window].sum()\n",
    "        for vector_type in ['raw', 'tfidf', 'pmi']:\n",
    "            if vector_type=='raw':\n",
    "                vecs = vectors[vocab, window]\n",
    "            if vector_type=='pmi':\n",
    "                vecs = get_pmi_vectors(vectors[vocab, window], N)\n",
    "            if vector_type=='tfidf' and vocab!='V':\n",
    "                vecs = get_idf_vecs(vectors[vocab, window], idfs[vocab])\n",
    "            if (vector_type, vocab) == ('tfidf', 'V'):\n",
    "                continue\n",
    "            for data_name, eval_data in zip(['MEN', 'SIMLEX'], [men, simlex]):\n",
    "                print(f\"\\n{vocab}, {data_name}, {window}, {vector_type}\")\n",
    "                rho, pval = evalws(vecs, eval_data, data_name)\n",
    "                scores_df.append(dict(zip(columns, [rho, pval, vocab, window, data_name, vector_type])),\n",
    "                                 ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_csv = \"\"\"vocab, test, window, method, rho, pval\n",
    "‘V_c’, ‘MEN’, 1, ‘raw’, 0.1453646412214026, 1.2324872951126856e-15\n",
    "‘V_c’, ‘MEN’, 1, ‘raw’, 0.05130971677729792, 0.10506450447826148\n",
    "‘V_c’, ‘MEN’, 1, ‘tfidf’, 0.29421919712354866, 5.612330898012024e-61\n",
    "‘V_c’, ‘SIMLEX’, 1, ‘tfidf’, 0.17777717010466934, 1.5396835417652116e-08\n",
    "‘V_c’, ‘MEN’, 1, ‘pmi’, 0.3622824597101258, 1.0183168811957188e-93\n",
    "‘V_c’, ‘SIMLEX’, 1, ‘pmi’, 0.20832696064990097, 2.942379302716302e-11\n",
    "‘V_c’, ‘MEN’, 3, ‘raw’, 0.2203416545296961, 2.6173677002997682e-34\n",
    "‘V_c’, ‘SIMLEX’, 3, ‘raw’, 0.05480605099111539, 0.08338305067574583\n",
    "’V_c’, MEN’, 3, ’tfidf’,  0.44886867503643035, 1.0831563340443806e-148\n",
    "‘V_c’, ‘SIMLEX’, 3, ‘tfidf’, 0.1855666928945375, 3.436639200620402e-09\n",
    "‘V_c’, ‘MEN’, 3, ‘pmi’, 0.46153842598967837, 3.549880350180334e-158\n",
    "‘V_c’, ‘SIMLEX’, 3, ‘pmi’, 0.19900072623832812, 2.2170010905021224e-10\n",
    "‘V_c’, ‘MEN’, 6, ‘raw’, 0.2405388357214604, 9.544733802667223e-41\n",
    "‘V_c’, ‘MEN’, 6, ‘raw’, 0.04655659821922139, 0.14143513010068376\n",
    "‘V_c’, ‘MEN’, 6, ‘tfidf’, 0.5196876901148884, 3.170819001168843e-207\n",
    "‘V_c’, ‘SIMLEX’, 6, ‘tfidf’, 0.11878414885025199, 0.0001678633374868749\n",
    "‘V_c’, ‘MEN’, 6, ‘pmi’, 0.4723553399745761, 1.3478358479482169e-166\n",
    "‘V_c’, ‘SIMLEX’, 6, ‘pmi’, 0.154171537123257, 9.776058133618332e-07\n",
    "‘V’, ‘MEN’, 1, ‘raw’, 0.14000684517560627, 1.3263418238349479e-14\n",
    "‘V’, ‘SIMLEX’, 1, ‘raw’, 0.05250729745927539, 0.09718379879864335\n",
    "‘V’, ‘MEN’, 1, ‘pmi’, 0.38785047256811156, 2.7867884669220316e-108\n",
    "‘V’, ‘SIMLEX’, 1, ‘pmi’, 0.23303023453015143, 8.712874018535476e-14\n",
    "‘V’, ‘MEN’, 3, ‘raw’, 0.21551090456727312, 7.352005167777831e-33\n",
    "‘V’, ‘SIMLEX’, 3, ‘raw’, 0.05361097703127073, 0.09034575849779028\n",
    "‘V’, ‘MEN’, 3, ‘pmi’, 0.5224845465220976, 7.833857588927124e-210\n",
    "‘V’, ‘SIMLEX’, 3, ‘pmi’, 0.22330635121502992, 9.364300306845717e-13\n",
    "‘V’, ‘MEN’, 6, ‘raw’, 0.23572152690209489, 3.731207056287394e-39\n",
    "‘V’, ‘SIMLEX’, 6, ‘raw’, 0.04227952379792869, 0.18179411276008164\n",
    "‘V’, ‘MEN’, 6, ‘pmi’, 0.5259332859986898, 4.418948548753325e-213\n",
    "‘V’, ‘SIMLEX’, 6, ‘pmi’, 0.16213999988025068, 2.570823255684684e-07\"\"\".strip(\"''\")\n",
    "\n",
    "with open('./scores_csv.csv', 'w') as f:\n",
    "    f.write(results_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vocab</th>\n",
       "      <th>test</th>\n",
       "      <th>window</th>\n",
       "      <th>method</th>\n",
       "      <th>rho</th>\n",
       "      <th>pval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‘V_c’</td>\n",
       "      <td>‘MEN’</td>\n",
       "      <td>1</td>\n",
       "      <td>‘raw’</td>\n",
       "      <td>0.145365</td>\n",
       "      <td>1.232487e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‘V_c’</td>\n",
       "      <td>‘MEN’</td>\n",
       "      <td>1</td>\n",
       "      <td>‘raw’</td>\n",
       "      <td>0.051310</td>\n",
       "      <td>1.050645e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>‘V_c’</td>\n",
       "      <td>‘MEN’</td>\n",
       "      <td>1</td>\n",
       "      <td>‘tfidf’</td>\n",
       "      <td>0.294219</td>\n",
       "      <td>5.612331e-61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‘V_c’</td>\n",
       "      <td>‘SIMLEX’</td>\n",
       "      <td>1</td>\n",
       "      <td>‘tfidf’</td>\n",
       "      <td>0.177777</td>\n",
       "      <td>1.539684e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‘V_c’</td>\n",
       "      <td>‘MEN’</td>\n",
       "      <td>1</td>\n",
       "      <td>‘pmi’</td>\n",
       "      <td>0.362282</td>\n",
       "      <td>1.018317e-93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>‘V_c’</td>\n",
       "      <td>‘SIMLEX’</td>\n",
       "      <td>1</td>\n",
       "      <td>‘pmi’</td>\n",
       "      <td>0.208327</td>\n",
       "      <td>2.942379e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>‘V_c’</td>\n",
       "      <td>‘MEN’</td>\n",
       "      <td>3</td>\n",
       "      <td>‘raw’</td>\n",
       "      <td>0.220342</td>\n",
       "      <td>2.617368e-34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>‘V_c’</td>\n",
       "      <td>‘SIMLEX’</td>\n",
       "      <td>3</td>\n",
       "      <td>‘raw’</td>\n",
       "      <td>0.054806</td>\n",
       "      <td>8.338305e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>’V_c’</td>\n",
       "      <td>MEN’</td>\n",
       "      <td>3</td>\n",
       "      <td>’tfidf’</td>\n",
       "      <td>0.448869</td>\n",
       "      <td>1.083156e-148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>‘V_c’</td>\n",
       "      <td>‘SIMLEX’</td>\n",
       "      <td>3</td>\n",
       "      <td>‘tfidf’</td>\n",
       "      <td>0.185567</td>\n",
       "      <td>3.436639e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>‘V_c’</td>\n",
       "      <td>‘MEN’</td>\n",
       "      <td>3</td>\n",
       "      <td>‘pmi’</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>3.549880e-158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>‘V_c’</td>\n",
       "      <td>‘SIMLEX’</td>\n",
       "      <td>3</td>\n",
       "      <td>‘pmi’</td>\n",
       "      <td>0.199001</td>\n",
       "      <td>2.217001e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>‘V_c’</td>\n",
       "      <td>‘MEN’</td>\n",
       "      <td>6</td>\n",
       "      <td>‘raw’</td>\n",
       "      <td>0.240539</td>\n",
       "      <td>9.544734e-41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>‘V_c’</td>\n",
       "      <td>‘MEN’</td>\n",
       "      <td>6</td>\n",
       "      <td>‘raw’</td>\n",
       "      <td>0.046557</td>\n",
       "      <td>1.414351e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>‘V_c’</td>\n",
       "      <td>‘MEN’</td>\n",
       "      <td>6</td>\n",
       "      <td>‘tfidf’</td>\n",
       "      <td>0.519688</td>\n",
       "      <td>3.170819e-207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>‘V_c’</td>\n",
       "      <td>‘SIMLEX’</td>\n",
       "      <td>6</td>\n",
       "      <td>‘tfidf’</td>\n",
       "      <td>0.118784</td>\n",
       "      <td>1.678633e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>‘V_c’</td>\n",
       "      <td>‘MEN’</td>\n",
       "      <td>6</td>\n",
       "      <td>‘pmi’</td>\n",
       "      <td>0.472355</td>\n",
       "      <td>1.347836e-166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>‘V_c’</td>\n",
       "      <td>‘SIMLEX’</td>\n",
       "      <td>6</td>\n",
       "      <td>‘pmi’</td>\n",
       "      <td>0.154172</td>\n",
       "      <td>9.776058e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>‘V’</td>\n",
       "      <td>‘MEN’</td>\n",
       "      <td>1</td>\n",
       "      <td>‘raw’</td>\n",
       "      <td>0.140007</td>\n",
       "      <td>1.326342e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>‘V’</td>\n",
       "      <td>‘SIMLEX’</td>\n",
       "      <td>1</td>\n",
       "      <td>‘raw’</td>\n",
       "      <td>0.052507</td>\n",
       "      <td>9.718380e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>‘V’</td>\n",
       "      <td>‘MEN’</td>\n",
       "      <td>1</td>\n",
       "      <td>‘pmi’</td>\n",
       "      <td>0.387850</td>\n",
       "      <td>2.786788e-108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>‘V’</td>\n",
       "      <td>‘SIMLEX’</td>\n",
       "      <td>1</td>\n",
       "      <td>‘pmi’</td>\n",
       "      <td>0.233030</td>\n",
       "      <td>8.712874e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>‘V’</td>\n",
       "      <td>‘MEN’</td>\n",
       "      <td>3</td>\n",
       "      <td>‘raw’</td>\n",
       "      <td>0.215511</td>\n",
       "      <td>7.352005e-33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>‘V’</td>\n",
       "      <td>‘SIMLEX’</td>\n",
       "      <td>3</td>\n",
       "      <td>‘raw’</td>\n",
       "      <td>0.053611</td>\n",
       "      <td>9.034576e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>‘V’</td>\n",
       "      <td>‘MEN’</td>\n",
       "      <td>3</td>\n",
       "      <td>‘pmi’</td>\n",
       "      <td>0.522485</td>\n",
       "      <td>7.833858e-210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>‘V’</td>\n",
       "      <td>‘SIMLEX’</td>\n",
       "      <td>3</td>\n",
       "      <td>‘pmi’</td>\n",
       "      <td>0.223306</td>\n",
       "      <td>9.364300e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>‘V’</td>\n",
       "      <td>‘MEN’</td>\n",
       "      <td>6</td>\n",
       "      <td>‘raw’</td>\n",
       "      <td>0.235722</td>\n",
       "      <td>3.731207e-39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>‘V’</td>\n",
       "      <td>‘SIMLEX’</td>\n",
       "      <td>6</td>\n",
       "      <td>‘raw’</td>\n",
       "      <td>0.042280</td>\n",
       "      <td>1.817941e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>‘V’</td>\n",
       "      <td>‘MEN’</td>\n",
       "      <td>6</td>\n",
       "      <td>‘pmi’</td>\n",
       "      <td>0.525933</td>\n",
       "      <td>4.418949e-213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>‘V’</td>\n",
       "      <td>‘SIMLEX’</td>\n",
       "      <td>6</td>\n",
       "      <td>‘pmi’</td>\n",
       "      <td>0.162140</td>\n",
       "      <td>2.570823e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    vocab       test   window    method       rho           pval\n",
       "0   ‘V_c’      ‘MEN’        1     ‘raw’  0.145365   1.232487e-15\n",
       "1   ‘V_c’      ‘MEN’        1     ‘raw’  0.051310   1.050645e-01\n",
       "2   ‘V_c’      ‘MEN’        1   ‘tfidf’  0.294219   5.612331e-61\n",
       "3   ‘V_c’   ‘SIMLEX’        1   ‘tfidf’  0.177777   1.539684e-08\n",
       "4   ‘V_c’      ‘MEN’        1     ‘pmi’  0.362282   1.018317e-93\n",
       "5   ‘V_c’   ‘SIMLEX’        1     ‘pmi’  0.208327   2.942379e-11\n",
       "6   ‘V_c’      ‘MEN’        3     ‘raw’  0.220342   2.617368e-34\n",
       "7   ‘V_c’   ‘SIMLEX’        3     ‘raw’  0.054806   8.338305e-02\n",
       "8   ’V_c’       MEN’        3   ’tfidf’  0.448869  1.083156e-148\n",
       "9   ‘V_c’   ‘SIMLEX’        3   ‘tfidf’  0.185567   3.436639e-09\n",
       "10  ‘V_c’      ‘MEN’        3     ‘pmi’  0.461538  3.549880e-158\n",
       "11  ‘V_c’   ‘SIMLEX’        3     ‘pmi’  0.199001   2.217001e-10\n",
       "12  ‘V_c’      ‘MEN’        6     ‘raw’  0.240539   9.544734e-41\n",
       "13  ‘V_c’      ‘MEN’        6     ‘raw’  0.046557   1.414351e-01\n",
       "14  ‘V_c’      ‘MEN’        6   ‘tfidf’  0.519688  3.170819e-207\n",
       "15  ‘V_c’   ‘SIMLEX’        6   ‘tfidf’  0.118784   1.678633e-04\n",
       "16  ‘V_c’      ‘MEN’        6     ‘pmi’  0.472355  1.347836e-166\n",
       "17  ‘V_c’   ‘SIMLEX’        6     ‘pmi’  0.154172   9.776058e-07\n",
       "18    ‘V’      ‘MEN’        1     ‘raw’  0.140007   1.326342e-14\n",
       "19    ‘V’   ‘SIMLEX’        1     ‘raw’  0.052507   9.718380e-02\n",
       "20    ‘V’      ‘MEN’        1     ‘pmi’  0.387850  2.786788e-108\n",
       "21    ‘V’   ‘SIMLEX’        1     ‘pmi’  0.233030   8.712874e-14\n",
       "22    ‘V’      ‘MEN’        3     ‘raw’  0.215511   7.352005e-33\n",
       "23    ‘V’   ‘SIMLEX’        3     ‘raw’  0.053611   9.034576e-02\n",
       "24    ‘V’      ‘MEN’        3     ‘pmi’  0.522485  7.833858e-210\n",
       "25    ‘V’   ‘SIMLEX’        3     ‘pmi’  0.223306   9.364300e-13\n",
       "26    ‘V’      ‘MEN’        6     ‘raw’  0.235722   3.731207e-39\n",
       "27    ‘V’   ‘SIMLEX’        6     ‘raw’  0.042280   1.817941e-01\n",
       "28    ‘V’      ‘MEN’        6     ‘pmi’  0.525933  4.418949e-213\n",
       "29    ‘V’   ‘SIMLEX’        6     ‘pmi’  0.162140   2.570823e-07"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df2 = pd.read_csv('./scores_csv.csv')\n",
    "\n",
    "scores_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of vocabulary, window size and method\n",
    "\n",
    "As the vocabulary size increases, we see a marginal (not large, but not insignificant) increase in the correlation. This can be explained by the additional context available from the increased number of words.\n",
    "\n",
    "As the window size increases, the correlations also increase. This can be explained by the larger number of context words we are leveraging for each usage of a word. For both V and V_c as context vocabularies, this increase is large from 1 to 3, but smaller from 3 to 6. This tells us that there is a decreasing incentive to increase the window size beyond a limit. This also makes intuitive sense.\n",
    "\n",
    "For small vocabularies and smaller windows, the PMI method dominates the TF-IDF one. However, with window size=6, TF-IDF surpasses PMI for the smaller context vocabulary.\n",
    "\n",
    "With the full vocabulary, PMI performs significantly well for all cases. I was unable to compute the TF-IDF vectors for this case because matrix multiplication of a 15000x15000 matrix with a 15000 column was prohibitively slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative differences between MEN and SIMLEX\n",
    "\n",
    "Consider these examples from MEN:\n",
    "\n",
    "```\n",
    "morning\tsunrise\t49.000000\n",
    "rain\tstorm\t49.000000\n",
    "festival\twhiskers\t1.000000\n",
    "muscle\ttulip\t1.000000\n",
    "```\n",
    "\n",
    "And the following from SIMLEX-999:\n",
    "\n",
    "```\n",
    "happy\tcheerful\t9.55\n",
    "hard\teasy\t0.95\n",
    "fast\trapid\t8.75\n",
    "stupid\tdumb\t9.58\n",
    "```\n",
    "\n",
    "The notion of similarity between MEN and SIMLEX seems to differ in that MEN classifies concepts or entities (mostly nouns) based on their similarity. On the other hand, SIMLEX-999 seems to record adjectives and their similarity/dissimilarity. In fact, SIMLEX-999 seems to have many synonym pairs with high scores, and antonym pairs with low scores.\n",
    "\n",
    "In general, this means we expect MEN correlations to be higher than SIMLEX ones. This is because the distributional hypothesis makes it difficult to infer the difference between synonyms and antonyms that are commonly used in the same contexts - e.g. This is a big/small house, A very tall/short man walked down the street, etc.\n",
    "\n",
    "This is clearly seen in the printed correlations. **Correlations on MEN are uniformly higher than those on SIMLEX.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 (5 points) For the two window sizes w = 1 and w = 6, compute and print the 10 nearest neighbors for the query word judges. (Hint: using my implementation, the nearest neighbor for both window sizes is judge, followed by justices for w = 1 and appeals with w = 6. Your nearest neighbor lists may differ slightly from mine, but hopefully these words are high up in your lists.)\n",
    "\n",
    "\n",
    "### 5.2 (10 points) Do nearest neighbors tend to have the same part-of-speech tag as the query word, or do they differ? Does the pattern differ across different part-of-speech tags for the query word? How does window size affect this? Explore these questions by choosing query words with different parts of speech and computing their nearest neighbors. When choosing query words, consider nouns, verbs, adjectives, and prepositions. (Hint: when considering verbs, use inflected forms like transported.) Try a few query words from each part of speech category and see if you can find any systematic patterns when comparing their nearest neighbors across window sizes 1 and 6. When the neighbors differ between window sizes, how do they differ? Can you find any query words that have almost exactly the same nearest neighbors with the two window sizes? Discuss your findings, showing examples of nearest neighbors for particular words to support your claims.\n",
    "\n",
    "\n",
    "### 5.3 (10 points) Now try choosing words with multiple senses (e.g., bank, cell, apple, apples, axes, frame, light, well, etc.) as query words. What appears to be happening with multisense words based on the nearest neighbors that you observe? What happens when you compare the neighbors with different window sizes (w = 1 vs. w = 6)? Discuss your findings, showing examples of nearest neighbors for particular words to support your claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Window = 1\n",
      "computing PMI vectors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d2eba25d59d4a37a8334e278c9138e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Window = 6\n",
      "computing PMI vectors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80731ce9318f4bbd8ca3ee15c79a61d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#pmi_vecs = {}\n",
    "for w in [1, 6]:\n",
    "    print(f'\\nWindow = {w}')\n",
    "    pmi_vecs[w] = get_pmi_vectors(vectors['V_c', w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbours(vecs, word, limit=10):\n",
    "    word_ind = word_to_ind[word]\n",
    "    wordvec = vecs[word_ind]\n",
    "    vecs = vecs.tocsr()\n",
    "    norms = norm(vecs, axis=1)\n",
    "    similarities = vecs.multiply(wordvec).sum(axis=1)\n",
    "    coeffs = np.array([1/(nrm*norms[word_ind]) if nrm!=0 else 0. for nrm in norms]).reshape((-1, 1))\n",
    "    similarities = np.multiply(similarities, coeffs).flatten().tolist()\n",
    "    args = np.argsort(similarities)[0][::-1]\n",
    "    neighbours = [V[arg] for arg in args[1:limit+1]]\n",
    "    nb_string = ', '.join(n for n in neighbours[:10])\n",
    "    print(f\"nearest 10 neighbours of {word}:\\n\\t{nb_string}\")\n",
    "    return neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "for w=1:\n",
      "nearest 10 neighbours of judges:\n",
      "\tjudge, players, justices, judiciary, members, courts, contestants, governments, officers, interceptions\n",
      "\n",
      "for w=6:\n",
      "nearest 10 neighbours of judges:\n",
      "\tjudge, courts, jury, supreme, justice, appeals, panel, contestants, judicial, candidates\n"
     ]
    }
   ],
   "source": [
    "for w in [1, 6]:\n",
    "    print(f\"\\nfor w={w}:\")\n",
    "    nearest_neighbours(pmi_vecs[w], 'judges', limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Window = 1\n",
      "nearest 10 neighbours of happy:\n",
      "\tpleased, proud, afraid, worried, comfortable, sad, sure, satisfied, sorry, confident\n",
      "nearest 10 neighbours of terrible:\n",
      "\tamazing, impressive, awful, weak, funny, blatant, beautiful, memorable, remarkable, tragic\n",
      "nearest 10 neighbours of talk:\n",
      "\town, main, second, last, article, page, talkpage, original, final, united\n",
      "nearest 10 neighbours of walk:\n",
      "\tbuy, hide, stay, get, reach, remember, write, leave, hear, remove\n",
      "\n",
      "Window = 6\n",
      "nearest 10 neighbours of happy:\n",
      "\tanyone, 'll, feel, everyone, ask, let, sure, saying, 'd, wants\n",
      "nearest 10 neighbours of terrible:\n",
      "\tsevere, worse, bad, telling, worst, thinks, happened, wrong, unfortunate, everyone\n",
      "nearest 10 neighbours of talk:\n",
      "\tpage, discussion, should, article, review, do, list, you, link, further\n",
      "nearest 10 neighbours of walk:\n",
      "\twalking, ride, foot, drive, journey, onto, trail, door, train, ball\n"
     ]
    }
   ],
   "source": [
    "for w in [1, 6]:\n",
    "    print(f'\\nWindow = {w}')\n",
    "    nearest_neighbours(pmi_vecs[w], 'happy', 10)\n",
    "    nearest_neighbours(pmi_vecs[w], 'terrible', 10)\n",
    "    nearest_neighbours(pmi_vecs[w], 'talk', 10)\n",
    "    nearest_neighbours(pmi_vecs[w], 'walk', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Window = 1\n",
      "nearest 10 neighbours of bank:\n",
      "\tbanks, side, province, coast, banking, railway, africa, shore, hemisphere, library\n",
      "nearest 10 neighbours of orange:\n",
      "\tblue, purple, pink, aluminum, yellow, red, green, coloured, bright, rain\n",
      "nearest 10 neighbours of term:\n",
      "\tterms, name, period, word, version, title, sentence, largest, language, paragraph\n",
      "nearest 10 neighbours of well:\n",
      "\timportant, so, popular, few, likely, far, common, small, but, same\n",
      "\n",
      "Window = 6\n",
      "nearest 10 neighbours of bank:\n",
      "\tcorporation, capital, railway, valley, southern, northern, centre, trade, banks, central\n",
      "nearest 10 neighbours of orange:\n",
      "\tyellow, green, blue, dark, bright, brown, color, purple, grey, colors\n",
      "nearest 10 neighbours of term:\n",
      "\tterms, word, example, subject, common, considered, language, meaning, means, words\n",
      "nearest 10 neighbours of well:\n",
      "\tsuch, other, many, most, some, are, have, like, more, all\n"
     ]
    }
   ],
   "source": [
    "for w in [1, 6]:\n",
    "    print(f'\\nWindow = {w}')\n",
    "    nearest_neighbours(pmi_vecs[w], 'bank', 10)\n",
    "    nearest_neighbours(pmi_vecs[w], 'orange', 10)\n",
    "    nearest_neighbours(pmi_vecs[w], 'term', 10)\n",
    "    nearest_neighbours(pmi_vecs[w], 'well', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Window = 1\n",
      "nearest 10 neighbours of transported:\n",
      "\tconverted, promoted, corrected, relegated, baptized, deported, resurrected, eliminated, overlooked, augmented\n",
      "nearest 10 neighbours of before:\n",
      "\tafter, during, ;, when, until, without, since, while, under, -\n",
      "nearest 10 neighbours of arrested:\n",
      "\tdeposed, beaten, convicted, diagnosed, kidnapped, banned, reunited, disqualified, murdered, acquitted\n",
      "nearest 10 neighbours of candidate:\n",
      "\tcandidates, party, nominee, nomination, caucus, coalition, leader, member, candidacy, parties\n",
      "\n",
      "Window = 6\n",
      "nearest 10 neighbours of transported:\n",
      "\tcargo, supplies, transport, carrying, passengers, ships, captured, vessels, equipment, transporting\n",
      "nearest 10 neighbours of before:\n",
      "\tafter, when, until, then, he, during, back, again, later, time\n",
      "nearest 10 neighbours of arrested:\n",
      "\tconvicted, murder, prison, sentenced, arrest, charged, attacked, killed, captured, killing\n",
      "nearest 10 neighbours of candidate:\n",
      "\tdemocratic, republican, candidates, election, nomination, liberal, elections, conservative, vote, seat\n"
     ]
    }
   ],
   "source": [
    "for w in [1, 6]:\n",
    "    print(f'\\nWindow = {w}')\n",
    "    nearest_neighbours(pmi_vecs[w], 'transported', 10)\n",
    "    nearest_neighbours(pmi_vecs[w], 'before', 10)\n",
    "    nearest_neighbours(pmi_vecs[w], 'arrested', 10)\n",
    "    nearest_neighbours(pmi_vecs[w], 'candidate', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do nearest neighbors tend to have the same part-of-speech tag as the query word, or do they differ?\n",
    "Nearest neighbours are not bound to have the same POS tag as the query word - but they frequently do. This is usually violated for words with multiple sense. With window=1, 'bank' has neighbours like 'library' as well as 'coast', indicating the two different senses. On the other hand, adjectives tend to have nearest neighbors that are also adjectives. This makes sense due to similar styles of word usage.\n",
    "\n",
    "\n",
    "#### Does the pattern differ across different part-of-speech tags for the query word? How does window size affect this? Explore these questions by choosing query words with different parts of speech and computing their nearest neighbors. When choosing query words, consider nouns, verbs, adjectives, and prepositions. (Hint: when considering verbs, use inflected forms like transported.) Try a few query words from each part of speech category and see if you can find any systematic patterns when comparing their nearest neighbors across window sizes 1 and 6.\n",
    "As mentioned, adjectives tend to have adjective neighbors. Similarly, as seen for prepositions and adverbs, these also tend to have neighbors that have the same POS tag. 'Before' is closest to 'after', 'when', 'until' and so on.\n",
    "\n",
    "With participles like 'arrested' or 'transported', we see some differences. As these can be used as both verbs and adjectives, we see a mix of neighbors. There are also related nouns in the list.\n",
    "\n",
    "\n",
    "\n",
    "#### When the neighbors differ between window sizes, how do they differ? Can you find any query words that have almost exactly the same nearest neighbors with the two window sizes? Discuss your findings, showing examples of nearest neighbors for particular words to support your claims.\n",
    "Often, we find that words with multiple senses (or POS) are represented better as the window size increases. This makes intuitive sense. In the example of 'bank', with window=1 we see limited contexts similar to the usage of 'river bank' - words like 'coast'. However, with increased window sizes, we see more trade related words as well.\n",
    "\n",
    "\n",
    "\n",
    "#### 5.3 (10 points) Now try choosing words with multiple senses (e.g., bank, cell, apple, apples, axes, frame, light, well, etc.) as query words. What appears to be happening with multisense words based on the nearest neighbors that you observe? What happens when you compare the neighbors with different window sizes (w = 1 vs. w = 6)? Discuss your findings, showing examples of nearest neighbors for particular words to support your claims.\n",
    "\n",
    "For words with multiple sense like 'talk', the nearest neighbors are often a mixed bag of different POS tags. It appears that these words get somewhat 'confused' by the distributional hypothesis, getting placed in the middle of many different contexts. For example, the nearest neighbors of 'bank' with w=1 show a mix of the two contexts in which the word can be used - as a river bank, or as the financial institution.\n",
    "\n",
    "Word sense disambiguation might help us by potentially creating different embeddings for different senses of a word, thereby resolving the confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
